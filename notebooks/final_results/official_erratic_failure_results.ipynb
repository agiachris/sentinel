{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "serif = False\n",
    "render_format = \"pdf\"\n",
    "if serif:\n",
    "    dir_postfix = \"\"\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"\n",
    "else:\n",
    "    dir_postfix = \"sans\"\n",
    "    plt.rcParams[\"font.family\"] = \"Liberation Sans\"\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "from results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erratic Failure Analysis: Close Box Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_exp_keys = []\n",
    "\n",
    "# Generate temporal consistency experiment keys.\n",
    "close_exp_keys += get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=[16],\n",
    "    sample_sizes=[32],\n",
    "    error_fns=[\"mmd_rbf_all\", \"kde_kl_all_rev\", \"kde_kl_all_for\", \"mse_all\"],\n",
    "    aggr_fns=[\"min\"],\n",
    ")\n",
    "\n",
    "# Generate loss function experiment keys.\n",
    "close_exp_keys += get_loss_function_exp_keys(\n",
    "    loss_fns=[\"noise_pred_all\", \"temporal_noise_pred_all\"],\n",
    "    sample_sizes=[10],\n",
    ")\n",
    "\n",
    "# Generate reconstruction experiment keys.\n",
    "close_exp_keys += get_loss_function_exp_keys(\n",
    "    loss_fns=[\"action_rec_all\", \"temporal_action_rec_all\"],\n",
    "    sample_sizes=[4],\n",
    ")\n",
    "\n",
    "# Generate embedding experiment keys.\n",
    "close_exp_keys += get_embedding_exp_keys(\n",
    "    embeddings=[\"encoder_feat\", \"clip_feat\", \"resnet_feat\"],\n",
    "    score_fns=[\"mahal\"],\n",
    ")\n",
    "\n",
    "# Generate ensemble experiment keys.\n",
    "close_exp_keys += get_ensemble_exp_keys(\n",
    "    pred_horizons=[16],\n",
    "    sample_sizes=[32],\n",
    "    action_spaces=[\"all\"],\n",
    ")\n",
    "\n",
    "# Load results (main result, over three seeds).\n",
    "close_metrics_0 = compile_metrics(\n",
    "    domain=\"0527_close_4\",\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=close_exp_keys,\n",
    "    return_test_data=True\n",
    ")\n",
    "close_aggr_metrics_0 = aggregate_metrics(\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=close_exp_keys,\n",
    "    data=close_metrics_0,\n",
    ")\n",
    "\n",
    "close_metrics_1 = compile_metrics(\n",
    "    domain=\"0528_close_4\",\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=close_exp_keys,\n",
    "    return_test_data=True,\n",
    ")\n",
    "close_aggr_metrics_1 = aggregate_metrics(\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=close_exp_keys,\n",
    "    data=close_metrics_1,\n",
    ")\n",
    "\n",
    "close_metrics_2 = compile_metrics(\n",
    "    domain=\"0529_close_4\",\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=close_exp_keys,\n",
    "    return_test_data=True,\n",
    ")\n",
    "close_aggr_metrics_2 = aggregate_metrics(\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=close_exp_keys,\n",
    "    data=close_metrics_2,\n",
    ")\n",
    "\n",
    "# Generate sentinel experiment keys.\n",
    "stac_exp_keys = get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=[16],\n",
    "    sample_sizes=[32],\n",
    "    error_fns=[\"mmd_rbf_all\"],\n",
    "    aggr_fns=[\"\"],\n",
    ")\n",
    "vlm_exp_keys = get_vlm_exp_keys(\n",
    "    models=[\n",
    "        \"gpt-4o\", \n",
    "        \"claude-3-5-sonnet-20240620\", \n",
    "        \"gemini-1-5-pro\"\n",
    "    ],\n",
    "    templates={\n",
    "        \"gpt-4o\": [\"image_qa\", \"video_qa\"],\n",
    "        \"claude-3-5-sonnet-20240620\": [\"image_qa\", \"video_qa\"],\n",
    "        \"gemini-1-5-pro\": [\"image_qa\", \"video_qa\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load results (VLM result, over one seed).\n",
    "close_metrics_3 = compile_metrics(\n",
    "    domain=\"0914_close_4\",\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=stac_exp_keys + vlm_exp_keys,\n",
    "    return_test_data=True,\n",
    "    return_test_frame=True,\n",
    ")\n",
    "close_aggr_metrics_3 = aggregate_metrics(\n",
    "    splits=[\"na\", \"hh\"],\n",
    "    exp_keys=stac_exp_keys + vlm_exp_keys,\n",
    "    data=close_metrics_3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Table Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key in close_exp_keys:\n",
    "    row = f\"{exp_key} & \"\n",
    "\n",
    "    # Dataset split result.\n",
    "    for split in [\"na\", \"hh\"]:\n",
    "        for metric in [\"TPR\", \"TNR\", \"TP Time Mean\"]:\n",
    "            stat = 0\n",
    "            denom = 0\n",
    "            \n",
    "            # Average metric over seeds.\n",
    "            for i, d in enumerate([close_metrics_0, close_metrics_1, close_metrics_2]):\n",
    "                if metric == \"TPR\" and d[split][exp_key][\"metrics\"][\"TPR\"] == 0 and i == 0:\n",
    "                    continue\n",
    "                elif metric == \"TP Time Mean\" and \"TP Time Mean\" not in d[split][exp_key][\"metrics\"]:\n",
    "                    continue\n",
    "                stat += d[split][exp_key][\"metrics\"][metric]\n",
    "                denom += 1\n",
    "\n",
    "            if denom == 0:\n",
    "                stat = -1\n",
    "            else:\n",
    "                stat = stat / denom\n",
    "            if metric == \"TP Time Mean\":\n",
    "                stat = stat / 5\n",
    "            row += f\"{round(stat, 2):0.2f} & \"\n",
    "        row += \"& \"\n",
    "    row += \"& \"\n",
    "    \n",
    "    # Aggregate result.\n",
    "    for metric in [\"TPR\", \"TNR\", \"Accuracy\"]:\n",
    "        stat = 0\n",
    "        denom = 0\n",
    "\n",
    "        # Average metric over seeds.\n",
    "        for i, d in enumerate([close_aggr_metrics_0, close_aggr_metrics_1, close_aggr_metrics_2]):\n",
    "            stat += d[exp_key][\"metrics\"][metric]\n",
    "            denom += 1\n",
    "\n",
    "        if denom == 0:\n",
    "            stat = -1\n",
    "        else:\n",
    "            stat = stat / denom\n",
    "        row += f\"{stat:0.2f} & \"\n",
    "    \n",
    "    print(row[:-2] + \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "for exp_key in vlm_exp_keys:\n",
    "    row = f\"{exp_key} & \"\n",
    "\n",
    "    # Dataset split result.\n",
    "    for split in [\"na\", \"hh\"]:\n",
    "        for metric in [\"TPR\", \"TNR\", \"TP Time Mean\"]:\n",
    "            stat = close_metrics_3[split][exp_key][\"metrics\"][metric]\n",
    "            if metric == \"TP Time Mean\":\n",
    "                stat = stat / 5\n",
    "            \n",
    "            row += f\"{round(stat, 2):0.2f} & \"\n",
    "        row += \"& & \"\n",
    "    \n",
    "    # Aggregate result.\n",
    "    for metric in [\"TPR\", \"TNR\", \"Accuracy\"]:\n",
    "        stat = close_aggr_metrics_3[exp_key][\"metrics\"][metric]\n",
    "        row += f\"{stat:0.2f} & \"\n",
    "    \n",
    "    print(row[:-2] + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentinel experiment keys.\n",
    "stac_exp_key = get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=[16],\n",
    "    sample_sizes=[32],\n",
    "    error_fns=[\"mmd_rbf_all\"],\n",
    "    aggr_fns=[\"\"],\n",
    ")[0]\n",
    "vlm_exp_keys = get_vlm_exp_keys(\n",
    "    models=[\"gpt-4o\"],\n",
    "    templates={\"gpt-4o\": [\"video_qa\"]}\n",
    ")\n",
    "\n",
    "# Compute sentinel results.\n",
    "_, _, _ = compute_sentinel_result(\n",
    "    stac_exp_key=stac_exp_key,\n",
    "    vlm_exp_keys_list=[vlm_exp_keys],\n",
    "    splits_list=[[\"na\", \"hh\"]],\n",
    "    metrics_list=[close_metrics_3],\n",
    "    time_mod=5.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_key = close_exp_keys[0]\n",
    "P_TOT = N_TOT = 0\n",
    "for split in [\"na\", \"hh\"]:\n",
    "    P = N = 0\n",
    "    for d in [close_metrics_0, close_metrics_1, close_metrics_2, close_metrics_3]:\n",
    "        labels: np.ndarray = d[split][exp_key][\"data\"][\"test_labels\"]\n",
    "        N += np.sum(labels == True)\n",
    "        P += np.sum(labels == False)\n",
    "    \n",
    "    print(f\"Split: {split} | Success: {N} | Failures: {P} | Rate: {N / (P + N):.2f}\")\n",
    "    N_TOT += N\n",
    "    P_TOT += P\n",
    "\n",
    "print(f\"Split: Combined | Success: {N_TOT} | Failures: {P_TOT} | Rate: {N_TOT / (P_TOT + N_TOT):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Result: Cumulative Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_exp_key = \"pred_horizon_16_sample_size_32_error_fn_mmd_rbf_all\"\n",
    "close_metrics_0_hh = compile_metrics(\n",
    "    domain=\"0527_close_4\",\n",
    "    splits=[\"hh\"],\n",
    "    exp_keys=[mmd_exp_key],\n",
    "    return_test_data=True,\n",
    "    return_test_frame=True,\n",
    "    return_demo_frame=True,\n",
    ")\n",
    "\n",
    "# Extract test scores.\n",
    "test_scores = get_detection_scores(\"hh\", mmd_exp_key, close_metrics_0_hh)\n",
    "p_scores = test_scores[\"P_scores\"][:, 2:]\n",
    "n_scores = test_scores[\"N_scores\"][:, 2:]\n",
    "upper = max(p_scores.max(), n_scores.max())\n",
    "lower = min(p_scores.min(), n_scores.min())\n",
    "\n",
    "# Normalize scores and compute quantiles.\n",
    "p_scores: np.ndarray = (p_scores - lower) / (upper - lower)\n",
    "n_scores: np.ndarray = (n_scores - lower) / (upper - lower)\n",
    "p_mean = p_scores.mean(axis=0)\n",
    "n_mean = n_scores.mean(axis=0)\n",
    "p_qh = np.quantile(p_scores, quantile, axis=0)\n",
    "n_qh = np.quantile(n_scores, quantile, axis=0)\n",
    "p_ql = np.quantile(p_scores, 1 - quantile, axis=0)\n",
    "n_ql = np.quantile(n_scores, 1 - quantile, axis=0)\n",
    "\n",
    "# Threshold\n",
    "demo_frame = close_metrics_0_hh[\"hh\"][mmd_exp_key][\"demo_frame\"]\n",
    "thresh = np.quantile(data_utils.aggr_episode_key_data(demo_frame, f\"{mmd_exp_key}_cum_score\"), quantile)\n",
    "thresh = (thresh - lower) / (upper - lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5.5))\n",
    "\n",
    "failure_color = \"orange\"\n",
    "success_color = \"blue\"\n",
    "thresh_color = \"red\"\n",
    "\n",
    "x = np.linspace(0, 1, len(p_mean))\n",
    "ax.plot(x, p_mean, label=\"Policy Failure\", color=failure_color, linewidth=4)\n",
    "ax.plot(x, n_mean, label=\"Policy Success\", color=success_color, linewidth=4)\n",
    "ax.fill_between(x, p_ql, p_qh, color=failure_color, alpha=0.15)\n",
    "ax.fill_between(x, n_ql, n_qh, color=success_color, alpha=0.15)\n",
    "ax.axhline(y=thresh, color=thresh_color, linestyle='--', linewidth=5, label=\"Detection Threshold\")\n",
    "\n",
    "ax.set_title(\"Temporal Consistency Score\", fontsize=26)\n",
    "ax.set_ylabel(\"Normalized Score ($\\\\eta_t$)\", fontsize=22)\n",
    "ax.set_ybound(0, 1.0)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "ax.set_xlabel(\"Normalized Trajectory Time (%)\", fontsize=22)\n",
    "xticks = np.linspace(0, 1, 6)\n",
    "xticklabels = [f\"{x:.1f}\" for x in xticks]\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xticklabels, fontsize=16)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(3)\n",
    "ax.spines['bottom'].set_linewidth(3)\n",
    "ax.legend(loc='upper left', fancybox=True, framealpha=0.7, fontsize=20, edgecolor='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = CWD / \"..\" / f\"figures_{dir_postfix}\" / f\"error-result.{render_format}\"\n",
    "plt.savefig(save_path, format=render_format, dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Sweep: Close Box Domain\n",
    "We conduct a hyperparameter sweep for the failure detector on a hold-out dataset. The selected hyperparameters must then generalize to the test sets above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate temporal consistency experiment keys.\n",
    "pred_horizons = [16]\n",
    "sample_sizes = [32, 64, 128, 256]\n",
    "\n",
    "# MMD sweeps.\n",
    "mmd_error_fns = [\n",
    "    \"mmd_rbf_all\",\n",
    "    \"mmd_rbf_all_median\",\n",
    "    \"mmd_rbf_all_eig\",\n",
    "    \"mmd_rbf_all_0.1\",\n",
    "    \"mmd_rbf_all_0.5\",\n",
    "    \"mmd_rbf_all_1.0\",\n",
    "    \"mmd_rbf_all_5.0\",\n",
    "    \"mmd_rbf_all_10.0\" ,\n",
    "    \"mmd_rbf_all_100.0\" ,\n",
    "]\n",
    "mmd_exp_keys = get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=pred_horizons,\n",
    "    sample_sizes=sample_sizes,\n",
    "    error_fns=mmd_error_fns,\n",
    "    aggr_fns=[\"min\"],\n",
    ")\n",
    "\n",
    "# KDE For. sweeps.\n",
    "kde_for_error_fns = [\n",
    "    \"kde_kl_all_for\",\n",
    "    \"kde_kl_all_for_eig\",\n",
    "    \"kde_kl_all_for_0.1\",\n",
    "    \"kde_kl_all_for_0.5\",\n",
    "    \"kde_kl_all_for_5.0\",\n",
    "    \"kde_kl_all_for_10.0\",\n",
    "    \"kde_kl_all_for_100.0\",\n",
    "]\n",
    "kde_for_exp_keys = get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=pred_horizons,\n",
    "    sample_sizes=sample_sizes,\n",
    "    error_fns=kde_for_error_fns,\n",
    "    aggr_fns=[\"min\"],\n",
    ")\n",
    "\n",
    "# KDE Rev.\n",
    "kde_rev_error_fns = [\n",
    "    \"kde_kl_all_rev\",\n",
    "    \"kde_kl_all_rev_eig\",\n",
    "    \"kde_kl_all_rev_0.1\",\n",
    "    \"kde_kl_all_rev_0.5\",\n",
    "    \"kde_kl_all_rev_5.0\",\n",
    "    \"kde_kl_all_rev_10.0\",\n",
    "    \"kde_kl_all_rev_100.0\",\n",
    "]\n",
    "kde_rev_exp_keys = get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=pred_horizons,\n",
    "    sample_sizes=sample_sizes,\n",
    "    error_fns=kde_rev_error_fns,\n",
    "    aggr_fns=[\"min\"],\n",
    ")\n",
    "\n",
    "# Compile metrics.\n",
    "mmd_metrics = compile_metrics(\n",
    "    domain=\"0525_close_4_sweep\",\n",
    "    splits=[\"na\", \"ll\", \"hh\"],\n",
    "    exp_keys=mmd_exp_keys,\n",
    ")\n",
    "mmd_metrics_aggr = aggregate_metrics(\n",
    "    splits=[\"na\", \"ll\", \"hh\"],\n",
    "    exp_keys=mmd_exp_keys,\n",
    "    data=mmd_metrics,\n",
    ")\n",
    "\n",
    "kde_for_metrics = compile_metrics(\n",
    "    domain=\"0525_close_4_sweep\",\n",
    "    splits=[\"na\", \"ll\", \"hh\"],\n",
    "    exp_keys=kde_for_exp_keys,\n",
    ")\n",
    "kde_for_metrics_aggr = aggregate_metrics(\n",
    "    splits=[\"na\", \"ll\", \"hh\"],\n",
    "    exp_keys=kde_for_exp_keys,\n",
    "    data=kde_for_metrics,\n",
    ")\n",
    "\n",
    "kde_rev_metrics = compile_metrics(\n",
    "    domain=\"0525_close_4_sweep\",\n",
    "    splits=[\"na\", \"ll\", \"hh\"],\n",
    "    exp_keys=kde_rev_exp_keys,\n",
    ")\n",
    "kde_rev_metrics_aggr = aggregate_metrics(\n",
    "    splits=[\"na\", \"ll\", \"hh\"],\n",
    "    exp_keys=kde_rev_exp_keys,\n",
    "    data=kde_rev_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMD result.\n",
    "for e, a in list(zip(*sort_metrics(exp_keys=mmd_exp_keys, data=mmd_metrics_aggr, metric=\"Balanced Accuracy\")))[:30]:\n",
    "    print(\"Method:\", e, \"| Score:\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE For. result.\n",
    "for e, a in list(zip(*sort_metrics(exp_keys=kde_for_exp_keys, data=kde_for_metrics_aggr, metric=\"Balanced Accuracy\")))[:30]:\n",
    "    print(\"Method:\", e, \"| Score:\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE Rev. result.\n",
    "for e, a in list(zip(*sort_metrics(exp_keys=kde_rev_exp_keys, data=kde_rev_metrics_aggr, metric=\"Balanced Accuracy\")))[:30]:\n",
    "    print(\"Method:\", e, \"| Score:\", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm-lfd-nhS_BEB0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
