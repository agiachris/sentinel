{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "serif = False\n",
    "render_format = \"pdf\"\n",
    "if serif:\n",
    "    dir_postfix = \"\"\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"\n",
    "else:\n",
    "    dir_postfix = \"sans\"\n",
    "    plt.rcParams[\"font.family\"] = \"Liberation Sans\"\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "from results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Progression Failure Analysis: Cover Object and Close Box Domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentinel experiment keys.\n",
    "vlm_exp_keys_cover = get_vlm_exp_keys(\n",
    "    models=[\"claude-3-5-sonnet-20240620\"],\n",
    "    templates = {\"claude-3-5-sonnet-20240620\": [\"video_qa\", \"video_qa_ref_video\", \"video_qa_ref_goal\"]}\n",
    ")\n",
    "vlm_exp_keys_close = get_vlm_exp_keys(\n",
    "    models=[\"gpt-4o\"],\n",
    "    templates={\"gpt-4o\": [\"video_qa\"]}\n",
    ")\n",
    "stac_exp_keys = get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=[16],\n",
    "    sample_sizes=[32],\n",
    "    error_fns=[\"mmd_rbf_all\"],\n",
    "    aggr_fns=[\"\"],\n",
    ")\n",
    "\n",
    "# Load results.\n",
    "cover_splits = [\"na\", \"ss\"]\n",
    "cover_metrics = compile_metrics(\n",
    "    domain=\"0914_cover_4\",\n",
    "    splits=cover_splits,\n",
    "    exp_keys=vlm_exp_keys_cover + stac_exp_keys,\n",
    "    return_test_data=True,\n",
    "    return_test_frame=True,\n",
    ")\n",
    "cover_metrics_aggr = aggregate_metrics(\n",
    "    splits=[\"na\", \"ss\"],\n",
    "    exp_keys=vlm_exp_keys_cover + stac_exp_keys,\n",
    "    data=cover_metrics\n",
    ")\n",
    "\n",
    "close_splits = [\"na\", \"ss\"]\n",
    "close_metrics = compile_metrics(\n",
    "    domain=\"0914_close_4\",\n",
    "    splits=close_splits,\n",
    "    exp_keys=vlm_exp_keys_close + stac_exp_keys,\n",
    "    return_test_data=True,\n",
    "    return_test_frame=True,\n",
    ")\n",
    "close_metrics_aggr = aggregate_metrics(\n",
    "    splits=[\"na\", \"ss\"],\n",
    "    exp_keys=vlm_exp_keys_close + stac_exp_keys,\n",
    "    data=close_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_key = stac_exp_keys[0]\n",
    "for domain, domain_metrics in zip([\"Cover\", \"Close\"], [[cover_metrics], [close_metrics]]):\n",
    "    print(f\"\\nDomain: {domain}\")\n",
    "    P_TOT = N_TOT = 0\n",
    "    for split in [\"na\", \"ss\"]:\n",
    "        P = N = 0\n",
    "        for d in domain_metrics:\n",
    "            labels: np.ndarray = d[split][exp_key][\"data\"][\"test_labels\"]\n",
    "            N += np.sum(labels == True)\n",
    "            P += np.sum(labels == False)\n",
    "        \n",
    "        print(f\"Split: {split} | Success: {N} | Failures: {P} | Rate: {N / (P + N):.2f}\")\n",
    "        N_TOT += N\n",
    "        P_TOT += P\n",
    "\n",
    "    print(f\"Split: Combined | Success: {N_TOT} | Failures: {P_TOT} | Rate: {N_TOT / (P_TOT + N_TOT):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDomain: Cover\")\n",
    "for exp_key in vlm_exp_keys_cover + stac_exp_keys:\n",
    "    metrics = cover_metrics_aggr[exp_key][\"metrics\"]\n",
    "    print_metrics(f\"{exp_key}\", metrics, with_accuracy=True, time_mod=5.0)\n",
    "\n",
    "print(\"\\nDomain: Close\")\n",
    "for exp_key in vlm_exp_keys_close + stac_exp_keys:\n",
    "    metrics = close_metrics_aggr[exp_key][\"metrics\"]\n",
    "    print_metrics(f\"{exp_key}\", metrics, with_accuracy=True, time_mod=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentinel results: Cover Object.\n",
    "stac_metrics_cover, vlme_metrics_cover, sent_metrics_cover = compute_sentinel_result(\n",
    "    stac_exp_key=stac_exp_keys[0],\n",
    "    vlm_exp_keys_list=[vlm_exp_keys_cover],\n",
    "    metrics_list=[cover_metrics],\n",
    "    splits_list=[cover_splits],\n",
    "    time_mod=5.0,\n",
    "    domain_names=[\"Cover\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentinel results: Close Box.\n",
    "stac_metrics_close, vlme_metrics_close, sent_metrics_close = compute_sentinel_result(\n",
    "    stac_exp_key=stac_exp_keys[0],\n",
    "    vlm_exp_keys_list=[vlm_exp_keys_close],\n",
    "    metrics_list=[close_metrics],\n",
    "    splits_list=[close_splits],\n",
    "    time_mod=5.0,\n",
    "    domain_names=[\"Close\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentinel results: Cover Object + Close Box.\n",
    "stac_metrics_joint, vlme_metrics_joint, sent_metrics_joint = compute_sentinel_result(\n",
    "    stac_exp_key=stac_exp_keys[0],\n",
    "    vlm_exp_keys_list=[vlm_exp_keys_cover, vlm_exp_keys_close],\n",
    "    metrics_list=[cover_metrics, close_metrics],\n",
    "    splits_list=[cover_splits, close_splits],\n",
    "    time_mod=5.0,\n",
    "    domain_names=[\"Cover\", \"Close\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_tpr = stac_metrics_joint[\"TPR\"]\n",
    "stac_fpr = stac_metrics_joint[\"FPR\"]\n",
    "stac_time = stac_metrics_joint[\"TP Time Mean\"] / 5.0\n",
    "\n",
    "vlme_tpr = vlme_metrics_joint[\"TPR\"]\n",
    "vlme_fpr = vlme_metrics_joint[\"FPR\"]\n",
    "vlme_time = vlme_metrics_joint[\"TP Time Mean\"] / 5.0\n",
    "\n",
    "sent_tpr = sent_metrics_joint[\"TPR\"]\n",
    "sent_fpr = sent_metrics_joint[\"FPR\"]\n",
    "sent_time = sent_metrics_joint[\"TP Time Mean\"] / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAC and Sentinel Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "colors = [\"#91c4a2\", \"#fdae61\"]\n",
    "labels = [\"Temporal Consistency + VLM\", \"Temporal Consistency\"]\n",
    "\n",
    "# Top: TPR.\n",
    "bar_width = 0.75\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "data1 = np.array([sent_tpr, stac_tpr])\n",
    "bars1 = ax1.barh(labels, data1, bar_width, color=colors)\n",
    "\n",
    "ax1.set_title(\"True Positive Rate\", fontsize=18)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_axisbelow(True)\n",
    "ax1.xaxis.grid(True, linestyle='-', linewidth=0.5, color=\"gray\")\n",
    "\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_linewidth(2.5)\n",
    "ax1.spines['bottom'].set_linewidth(2.5)\n",
    "ax1.tick_params(axis='x', labelsize=13)\n",
    "\n",
    "distance = np.abs(stac_tpr - sent_tpr)\n",
    "ax1.annotate(\n",
    "    \"\",\n",
    "    xy=(sent_tpr - 0.02, 0.9), xytext=(stac_tpr + 0.02, 0.9),\n",
    "    xycoords='data', textcoords=\"data\",\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<->\", \n",
    "        \"ec\": \"black\",\n",
    "        \"linewidth\": 2.0,\n",
    "    }\n",
    ")\n",
    "ax1.annotate(\n",
    "    f\"+{int(distance * 100)}%\",\n",
    "    xy=((stac_tpr + sent_tpr) / 2 - 0.05, 1.1), xytext=(0, 0),\n",
    "    xycoords=\"data\", textcoords=\"offset points\", fontsize=14\n",
    ")\n",
    "\n",
    "# Left: FPR.\n",
    "bar_width = 0.7\n",
    "ax2 = plt.subplot(2, 2, 3)\n",
    "data2 = np.array([sent_fpr, stac_fpr])\n",
    "bars2 = ax2.barh(labels, data2, bar_width, color=colors)\n",
    "\n",
    "ax2.set_title(\"False Positive Rate\", fontsize=18)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_yticklabels([])\n",
    "ax2.set_xlim([0, 0.2])\n",
    "ax2.set_xticks([0, 0.05, 0.1, 0.15, 0.2])\n",
    "ax2.set_axisbelow(True)\n",
    "ax2.xaxis.grid(True, linestyle='-', linewidth=0.5, color=\"gray\")\n",
    "ax2.tick_params(axis='x', labelsize=13)\n",
    "\n",
    "distance = np.abs(stac_fpr - sent_fpr)\n",
    "ax2.annotate(\n",
    "    \"\",\n",
    "    xy=(sent_fpr, 0.9), xytext=(stac_fpr, 0.9),\n",
    "    xycoords='data', textcoords=\"data\",\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<->\", \n",
    "        \"ec\": \"black\",\n",
    "        \"linewidth\": 2.0,\n",
    "    }\n",
    ")\n",
    "ax2.annotate(\n",
    "    f\"+{int(distance * 100)}%\",\n",
    "    xy=((stac_fpr + sent_fpr) / 2 - 0.018, 1.1), xytext=(0, 0),\n",
    "    xycoords=\"data\", textcoords=\"offset points\", fontsize=13\n",
    ")\n",
    "\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['left'].set_linewidth(2.5)\n",
    "ax2.spines['bottom'].set_linewidth(2.5)\n",
    "\n",
    "# Right: Detection Times.\n",
    "bar_width = 0.7\n",
    "ax3 = plt.subplot(2, 2, 4)\n",
    "data3 = np.array([sent_time, stac_time])\n",
    "bars3 = ax3.barh(labels, data3, bar_width, color=colors)\n",
    "\n",
    "ax3.set_title(\"Detection Time (s)\", fontsize=18)\n",
    "ax3.set_yticks([])\n",
    "ax3.set_yticklabels([])\n",
    "ax3.set_axisbelow(True)\n",
    "ax3.xaxis.grid(True, linestyle='-', linewidth=0.5, color=\"gray\")\n",
    "ax3.tick_params(axis='x', labelsize=13)\n",
    "\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.spines['left'].set_linewidth(2.5)\n",
    "ax3.spines['bottom'].set_linewidth(2.5)\n",
    "\n",
    "fig.legend(bars1[::-1], labels[::-1], loc='upper center', bbox_to_anchor=(0.5, 0.025), ncol=2, fancybox=True, fontsize=13)\n",
    "plt.subplots_adjust(bottom=0.2, wspace=1.0)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = CWD / \"..\" / f\"figures_{dir_postfix}\" / f\"full-system-result.{render_format}\"\n",
    "plt.savefig(save_path, format=render_format, dpi=300, bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAC, VLM, and Sentinel Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "colors = [\"#91c4a2\", \"#c994c7\", \"#fdae61\"]\n",
    "labels = [\"VLM (GPT-4o / Claude)\", \"Sentinel (STAC + VLM)\", \"STAC\"]\n",
    "\n",
    "# Top: TPR.\n",
    "bar_width = 0.75\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "data1 = np.array([vlme_tpr, sent_tpr, stac_tpr])\n",
    "bars1 = ax1.barh(labels, data1, bar_width, color=colors)\n",
    "\n",
    "ax1.set_title(\"True Positive Rate\", fontsize=18)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_axisbelow(True)\n",
    "ax1.xaxis.grid(True, linestyle='-', linewidth=0.5, color=\"gray\")\n",
    "\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_linewidth(2.5)\n",
    "ax1.spines['bottom'].set_linewidth(2.5)\n",
    "ax1.tick_params(axis='x', labelsize=13)\n",
    "\n",
    "distance = np.abs(stac_tpr - sent_tpr)\n",
    "ax1.annotate(\n",
    "    \"\",\n",
    "    xy=(sent_tpr - 0.02, 1.8), xytext=(stac_tpr + 0.02, 1.8),\n",
    "    xycoords='data', textcoords=\"data\",\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<->\", \n",
    "        \"ec\": \"black\",\n",
    "        \"linewidth\": 2.0,\n",
    "    }\n",
    ")\n",
    "ax1.annotate(\n",
    "    f\"+{int(distance * 100)}%\",\n",
    "    xy=((stac_tpr + sent_tpr) / 2 - 0.05, 2.0), xytext=(0, 0),\n",
    "    xycoords=\"data\", textcoords=\"offset points\", fontsize=14\n",
    ")\n",
    "\n",
    "# Left: FPR.\n",
    "bar_width = 0.7\n",
    "ax2 = plt.subplot(2, 2, 3)\n",
    "data2 = np.array([vlme_fpr, sent_fpr, stac_fpr])\n",
    "bars2 = ax2.barh(labels, data2, bar_width, color=colors)\n",
    "\n",
    "ax2.set_title(\"False Positive Rate\", fontsize=18)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_yticklabels([])\n",
    "ax2.set_xlim([0, 0.2])\n",
    "ax2.set_xticks([0, 0.05, 0.1, 0.15, 0.2])\n",
    "ax2.set_axisbelow(True)\n",
    "ax2.xaxis.grid(True, linestyle='-', linewidth=0.5, color=\"gray\")\n",
    "ax2.tick_params(axis='x', labelsize=13)\n",
    "\n",
    "distance = np.abs(stac_fpr - sent_fpr)\n",
    "ax2.annotate(\n",
    "    \"\",\n",
    "    xy=(sent_fpr, 1.8), xytext=(stac_fpr, 1.8),\n",
    "    xycoords='data', textcoords=\"data\",\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<->\", \n",
    "        \"ec\": \"black\",\n",
    "        \"linewidth\": 2.0,\n",
    "    }\n",
    ")\n",
    "ax2.annotate(\n",
    "    f\"+{int(distance * 100)}%\",\n",
    "    xy=((stac_fpr + sent_fpr) / 2 - 0.018, 2.0), xytext=(0, 0),\n",
    "    xycoords=\"data\", textcoords=\"offset points\", fontsize=13\n",
    ")\n",
    "\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['left'].set_linewidth(2.5)\n",
    "ax2.spines['bottom'].set_linewidth(2.5)\n",
    "\n",
    "# Right: Detection Times.\n",
    "bar_width = 0.7\n",
    "ax3 = plt.subplot(2, 2, 4)\n",
    "data3 = np.array([vlme_time, sent_time, stac_time])\n",
    "bars3 = ax3.barh(labels, data3, bar_width, color=colors)\n",
    "\n",
    "ax3.set_title(\"Detection Time (s)\", fontsize=18)\n",
    "ax3.set_yticks([])\n",
    "ax3.set_yticklabels([])\n",
    "ax3.set_axisbelow(True)\n",
    "ax3.xaxis.grid(True, linestyle='-', linewidth=0.5, color=\"gray\")\n",
    "ax3.tick_params(axis='x', labelsize=13)\n",
    "\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.spines['left'].set_linewidth(2.5)\n",
    "ax3.spines['bottom'].set_linewidth(2.5)\n",
    "\n",
    "\n",
    "fig.legend(bars1[::-1], labels[::-1], loc='upper center', bbox_to_anchor=(0.5, 0.025), ncol=3, fancybox=True, fontsize=12)\n",
    "plt.subplots_adjust(bottom=0.2, wspace=1.0)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = CWD / \"..\" / f\"figures_{dir_postfix}\" / f\"full-system-result.{render_format}\"\n",
    "plt.savefig(save_path, format=render_format, dpi=300, bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Table Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentinel experiment keys.\n",
    "vlm_exp_keys_cover = get_vlm_exp_keys(\n",
    "    models=[\n",
    "        \"gpt-4o\", \n",
    "        \"claude-3-5-sonnet-20240620\", \n",
    "        \"gemini-1-5-pro\"\n",
    "    ],\n",
    "    templates={\n",
    "        \"gpt-4o\": [\"image_qa\", \"video_qa\"],\n",
    "        \"claude-3-5-sonnet-20240620\": [\"image_qa\", \"video_qa\", \"video_qa_ref_video\", \"video_qa_ref_goal\"],\n",
    "        \"gemini-1-5-pro\": [\"image_qa\", \"video_qa\"],\n",
    "    }\n",
    ")\n",
    "vlm_exp_keys_close = get_vlm_exp_keys(\n",
    "    models=[\n",
    "        \"gpt-4o\", \n",
    "        \"claude-3-5-sonnet-20240620\", \n",
    "        \"gemini-1-5-pro\"\n",
    "    ],\n",
    "    templates={\n",
    "        \"gpt-4o\": [\"image_qa\", \"video_qa\"],\n",
    "        \"claude-3-5-sonnet-20240620\": [\"image_qa\", \"video_qa\"],\n",
    "        \"gemini-1-5-pro\": [\"image_qa\", \"video_qa\"],\n",
    "    }\n",
    ")\n",
    "stac_exp_keys = get_temporal_consistency_exp_keys(\n",
    "    pred_horizons=[16],\n",
    "    sample_sizes=[32],\n",
    "    error_fns=[\"mmd_rbf_all\", \"kde_kl_all_rev\", \"kde_kl_all_for\", \"mse_all\"],\n",
    "    aggr_fns=[\"min\"],\n",
    ")\n",
    "\n",
    "# Generate ensemble experiment keys.\n",
    "ens_exp_keys = get_ensemble_exp_keys(\n",
    "    pred_horizons=[16],\n",
    "    sample_sizes=[32],\n",
    "    action_spaces=[\"all\"],\n",
    ")\n",
    "\n",
    "# Load results.\n",
    "cover_splits = [\"na\", \"ss\"]\n",
    "cover_metrics = compile_metrics(\n",
    "    domain=\"0914_cover_4\",\n",
    "    splits=cover_splits,\n",
    "    exp_keys=vlm_exp_keys_cover + stac_exp_keys + ens_exp_keys,\n",
    "    return_test_data=True,\n",
    "    return_test_frame=True,\n",
    ")\n",
    "cover_metrics_aggr = aggregate_metrics(\n",
    "    splits=[\"na\", \"ss\"],\n",
    "    exp_keys=vlm_exp_keys_cover + stac_exp_keys + ens_exp_keys,\n",
    "    data=cover_metrics\n",
    ")\n",
    "\n",
    "close_splits = [\"na\", \"ss\"]\n",
    "close_metrics = compile_metrics(\n",
    "    domain=\"0914_close_4\",\n",
    "    splits=close_splits,\n",
    "    exp_keys=vlm_exp_keys_close + stac_exp_keys + ens_exp_keys,\n",
    "    return_test_data=True,\n",
    "    return_test_frame=True,\n",
    ")\n",
    "close_metrics_aggr = aggregate_metrics(\n",
    "    splits=[\"na\", \"ss\"],\n",
    "    exp_keys=vlm_exp_keys_close + stac_exp_keys + ens_exp_keys,\n",
    "    data=close_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key in stac_exp_keys + vlm_exp_keys_cover + ens_exp_keys:\n",
    "    row = f\"{exp_key} & \"\n",
    "    \n",
    "    # Dataset split result.\n",
    "    for split in cover_splits:\n",
    "        for metric in [\"TPR\", \"TNR\", \"TP Time Mean\"]:\n",
    "            stat = cover_metrics[split][exp_key][\"metrics\"].get(metric, -1)\n",
    "            if stat < 0:\n",
    "                row += \"N/A & \"\n",
    "                continue\n",
    "            elif metric == \"TP Time Mean\":\n",
    "                stat = stat / 5\n",
    "            row += f\"{round(stat, 2):0.2f} & \"\n",
    "        row += \"& \"\n",
    "    row += \"& \"\n",
    "\n",
    "    # Aggregate result.\n",
    "    for metric in [\"TPR\", \"TNR\", \"Accuracy\"]:\n",
    "        stat = cover_metrics_aggr[exp_key][\"metrics\"][metric]\n",
    "        row += f\"{stat:0.2f} & \"\n",
    "\n",
    "    print(row[:-2] + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key in stac_exp_keys + vlm_exp_keys_close + ens_exp_keys:\n",
    "    row = f\"{exp_key} & \"\n",
    "    \n",
    "    # Dataset split result.\n",
    "    for split in close_splits:\n",
    "        for metric in [\"TPR\", \"TNR\", \"TP Time Mean\"]:\n",
    "            stat = close_metrics[split][exp_key][\"metrics\"].get(metric, -1)\n",
    "            if stat < 0:\n",
    "                row += \"N/A & \"\n",
    "                continue\n",
    "            elif metric == \"TP Time Mean\":\n",
    "                stat = stat / 5\n",
    "            row += f\"{round(stat, 2):0.2f} & \"\n",
    "        row += \"& \"\n",
    "    row += \"& \"\n",
    "\n",
    "    # Aggregate result.\n",
    "    for metric in [\"TPR\", \"TNR\", \"Accuracy\"]:\n",
    "        stat = close_metrics_aggr[exp_key][\"metrics\"][metric]\n",
    "        row += f\"{stat:0.2f} & \"\n",
    "\n",
    "    print(row[:-2] + \"\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm-lfd-nhS_BEB0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
